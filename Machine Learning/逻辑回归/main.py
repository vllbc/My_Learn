import numpy as np


class LogisticRegression:
    def __init__(self):
        pass
    def sigmoid(self,a):
        res = []
        for x in a:
            if x >= 0:
                res.append(1/(1+np.exp(-x)))
            else:
                res.append(np.exp(x) / (np.exp(x) + 1))
        return np.array(res)
    def train(self, X, y_true, n_iters=100, learning_rate=1):
        """
        æ ¹æ®ç»™å®šçš„è®­ç»ƒé›†Xå’Œyæ¥è®­ç»ƒé€»è¾‘å›å½’
        """
        # ç¬¬é›¶æ­¥ï¼šåˆå§‹åŒ–å‚æ•°
        n_samples, n_features = X.shape
        #ğŸ‘†æ ·æœ¬æ•°må’Œç‰¹å¾é‡æ•°nåˆ†åˆ«èµ‹å€¼ä¸ºXçš„è¡Œæ•°å’Œåˆ—æ•°
        self.weights = np.zeros((n_features,1))
        self.bias = 0
        costs = []

        for i in range(n_iters):
            # ç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥ï¼šè®¡ç®—è¾“å…¥çš„ç‰¹å¾é‡å’Œæƒå€¼çš„çº¿æ€§ç»„åˆï¼Œä½¿ç”¨sigmoidå‡½æ•°
            y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias)
            # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—ä»£ä»·å€¼ï¼Œç”¨äºä¹‹åè®¡ç®—ä»£ä»·å‡½æ•°å€¼
            cost = (-1/n_samples)*np.sum(y_true*np.log(y_predict+1e-5)+(1-y_true)*(np.log(1-y_predict+1e-5)))
            # ç¬¬å››æ­¥ï¼šè®¡ç®—æ¢¯åº¦
            dw = (1/n_samples)*np.dot(X.T,(y_predict - y_true))
            db = (1/n_samples)*np.sum(y_predict-y_true)
            # ç¬¬äº”æ­¥ï¼›æ›´æ–°å‚æ•°
            self.weights = self.weights - learning_rate * dw
            self.bias = self.bias - learning_rate * db

            costs.append(cost)
            if i%10 == 0:
                print(f"Cost after iteration {i}:{cost}")

        # return self.weights,self.bias,costs

    def predict(self,X):
        """
        å¯¹äºæµ‹è¯•é›†Xï¼Œé¢„æµ‹äºŒå…ƒåˆ†ç±»æ ‡ç­¾
        """
        y_predict = self.sigmoid(np.dot(X,self.weights)+self.bias)
        return np.array(y_predict)