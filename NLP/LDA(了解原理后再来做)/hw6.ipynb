{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_hdacO7y0DA",
    "outputId": "9d1759e5-3b56-49f1-99fc-5782bb5b1664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Collecting pysnooper\n",
      "  Downloading https://pypi.doubanio.com/packages/8f/eb/563ef82f216d6d315a0ab9f2bace4ae36e5c17b883651d8c726db541d1a6/PySnooper-1.1.0-py2.py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pysnooper\n",
      "Successfully installed pysnooper-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# this is a tool for debugging in python that you can use it for debugging in this assignment. \n",
    "# when you finished debugging, please remember to delete the decorater @pysnooper.snoop on top of the functions. \n",
    "!pip install pysnooper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3V0KFjzy37F"
   },
   "source": [
    "# STAT4609 Assignment 6:  Gibbs sampling algorithm for Latent Dirichlet Allocation. \n",
    "\n",
    "In this assignment, we are going to understand the idea of latent Dirichlet allocation (LDA), a flexible model to estimate the properties of text. On the example of\n",
    "LDA, the usage of Gibbs sampling is shown as a straight-forward means of approximate\n",
    "inference in Bayesian networks. \n",
    "\n",
    "\n",
    "## Part 1: Reading and Short Essay\n",
    "\n",
    "### Please read the following paper via [this link](https://www.pnas.org/content/101/suppl_1/5228).\n",
    "\n",
    "    Griffiths, Thomas L., and Mark Steyvers. \"Finding scientific topics.\" Proceedings of the National academy of Sciences 101.suppl 1 (2004): 5228-5235.\n",
    "\n",
    "\n",
    "\n",
    "The notations in the aforementioned reference varies. Please be careful when you are reading. \n",
    "\n",
    "\n",
    "\n",
    "### Please write a short essay to describe the following subjects, \n",
    "1. The probabilistic graphcial models and dependencies for LDA model. Please define notations before refering to.  \n",
    "2. The Gibbs sampling update steps for the parameters of LDA. Deriving the update steps should be straightforward after writing the dependencies of the parameters in the graphical model.\n",
    "3. The goal of Gibbs Sampling for the inference Latent Dirichlet Allocation? \n",
    "\n",
    "\n",
    "**Optional**\n",
    "\n",
    "If you would like to know more about the mathematical and technical details, please read **Section 5**, [Parameter estimation for text analysis](http://www.arbylon.net/publications/text-est.pdf).\n",
    "\n",
    "Please note that all the comments and notations in this jupyter notebook follows this reference for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CQjCSzVuy4Dw"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZjc7h-kDONc"
   },
   "source": [
    "## Part 2 Implementation of LDA and Inference via Gibbs sampling for LDA \n",
    "Although latent Dirichlet allocation is still a relatively simple model, exact inference is generally intractable. The solution to this is to use approximate inference algorithms. Gibbs sampling is a special case of Markov-chain Monte Carlo (MCMC) simulation and often yields relatively simple algorithms for approximate inference in high-dimensional models such as LDA. Therefore we select this approach and present a derivation that is more detailed than the original one by Griffiths and Steyvers. \n",
    "\n",
    "\n",
    "To derive a Gibbs sampler for LDA, we apply the hidden-variable method from above. The hidden variables in our model are $z_{d, w}$, i.e., the topic that appear with the word $w$ in document $d$ . We do not need to include, i.e., can integrate out, the parameter sets $\\Theta$ and $\\Phi$ because they can be interpreted as statistics of the associations between the observed word and the corresponding topic, the state variables of the Markov chain. \n",
    "\n",
    "In this assignment, we will use Gibbs sampling for LDA. \n",
    "distribution. During the inference, the distribution of topics in docuemnts, $\\mathbf{\\theta}$, the distribution of words in topics $\\mathbf{\\phi}$, and likelihood \n",
    "$p(\\mathbf{w}|\\mathbf{z})$ are estimated. \n",
    "\n",
    "\n",
    "### Settings\n",
    "1. we have $T$ topics\n",
    "2. The copus are consist of $D$ documents, and there are $W$ words within each document $d$. \n",
    "3. For dirichlet distribution of $\\mathbf{\\phi}$, the the number of prior parameters are equal to the number of vocabulary. The prior parameters are all of the same value, $\\beta$. \n",
    "4. For dirichlet distribution of $\\mathbf{\\theta}$, the the number of prior parameters are equal to the number of topics. The prior parameters are all of the same value, $\\alpha$. \n",
    "\n",
    "###  Argument Definitions and Types \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "z: z must take in (d,w,i) as input, corresponding to \n",
    "    topic indicator for i-th obserevation of word w in doc d.\n",
    "    z is a python dictionary. \n",
    "```\n",
    "```\n",
    "topic_word_matrix: of dimension (T, W).  \n",
    "    It refers to the topic of each word. t-th row of topic_word_matrix corresponds to the word observation counts for topic $t$. \n",
    "\n",
    "```     \n",
    "In the first reference, ```topic_word_matrix[j, w]``` is noted as $n_j^{(w)}$, which is the number of times word $w$ has been assigned\n",
    "to topic $j$. \n",
    "\n",
    "\n",
    "```\n",
    "doc_topic_matrix: of dimension (D, T). It refers to the topic counts for each document. \n",
    "\n",
    "```\n",
    "```doc_topic_matrix[d, j]``` is noted as $n_j^{(d)}$ the number of times a word from document $d$ has been assigned to topic $j$.  \n",
    "\n",
    "\n",
    "```\n",
    "topic_counts: the total number of words has been assigned to each topic. \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "doc_counts: document topic sum \n",
    "```\n",
    "\n",
    "\n",
    "log_likelihood corresponding to the $log(p(\\mathbf{w}|\\mathbf{z}))$. $p(\\mathbf{w}|\\mathbf{z})$ is equation [2] in the first reference. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "MdNzvsdeJVJG",
    "outputId": "5b63044c-d0c7-47da-a35a-932fedc4ebf7"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "LDA implementation in Python\n",
    "\n",
    "@author: Michael Zhang\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pysnooper\n",
    "\n",
    "class LDA(object):\n",
    "    \n",
    "    def __init__(self, tdm, T, alpha = 1., beta=1., iteration=100):\n",
    "        \"\"\"\n",
    "        tdm: the copus, of (D * Num_words_in_corpus), \n",
    "            the value of each entry is the counts of corresponding words in this the corresponding document.\n",
    "            e.g.\n",
    "            tdm[d, w] = number of word w appears in document d. \n",
    "        T: the number of topics \n",
    "        \n",
    "        \"\"\"\n",
    "        self.tdm = tdm\n",
    "        self.D, self.W = self.tdm.shape                \n",
    "        self.alpha= alpha\n",
    "        self.beta = beta\n",
    "        self.T = T \n",
    "        self.iteration = iteration\n",
    "        \n",
    "        # z must take in (d,w,i) as input, corresponding to \n",
    "        # topic indicator for i-th obserevation of word w in doc d\n",
    "        self.z = {} \n",
    "        self.topic_word_matrix = np.zeros((self.T, self.W)) \n",
    "        self.doc_topic_matrix = np.zeros((self.D, self.T))\n",
    "        self.topic_counts = np.zeros(self.T)\n",
    "        self.doc_counts = np.zeros(self.D)\n",
    "        self.log_likelihood = np.zeros(self.iteration)\n",
    "        self._init_matrix()\n",
    "        \n",
    "    # @pysnooper.snoop('init.log')    \n",
    "    def _init_matrix(self):\n",
    "        \"\"\"\n",
    "        for all words\n",
    "        1. sample a topic randomly from T topics for each word \n",
    "        2. increment topic word count, self.topic_word_matrix\n",
    "        3. increment document topic count,  self.doc_topic_matrix \n",
    "        4. update the topic indicator z. \n",
    "        \"\"\"\n",
    "        for d in range(self.D):\n",
    "            doc = scipy.sparse.coo_matrix(self.tdm[d])\n",
    "            word_freq_topic = zip(doc.col, doc.data)\n",
    "            for w, frequency in word_freq_topic:\n",
    "                for i in range(frequency):\n",
    "                    ############ Finish the following initialization steps #############\n",
    "                    # 1. sample a topic randomly from T topics for each word \n",
    "                    topic = np.random.randint(self.T)\n",
    "                    # 2. increment topic word count, self.topic_word_matrix\n",
    "                    ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                    # 3. increment document topic count,  self.doc_topic_matrix \n",
    "                    ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                    # 4. update the topic indicator z. \n",
    "                    ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                    ###################################################################        \n",
    "        self.topic_counts = self.topic_word_matrix.sum(axis=1)\n",
    "        self.doc_counts = self.doc_topic_matrix.sum(axis=1)\n",
    "   \n",
    "    # @pysnooper.snoop('fit.log')    \n",
    "    def fit(self):\n",
    "        for it in range(self.iteration):\n",
    "            # iterate over all the documents\n",
    "            for d in range(self.D):\n",
    "            # iterate over all the words in d\n",
    "                for w in self.tdm[d].indices: \n",
    "                    # iterate over number of times observed word w in doc d\n",
    "                    for i in range(self.tdm[d,w]):\n",
    "                        # we apply the hidden-varible method of Gibbs sampler, the hidden variable is z[(d,w,i)]\n",
    "                        self.doc_topic_matrix[d,self.z[(d,w,i)]] -= 1\n",
    "                        self.doc_counts[d] -= 1\n",
    "                        self.topic_word_matrix[self.z[(d,w,i)],w] -= 1\n",
    "                        self.topic_counts[self.z[(d,w,i)]] -= 1\n",
    "                        ######++++++++++++++++++++++++++++++++++++++########\n",
    "                        # estimation of phi and theta for the current corpus \n",
    "                        # according to equation [6] [7]\n",
    "                        #\n",
    "                        phi_hat = ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                        theta_hat =## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "\n",
    "                        # calculate the full conditional distribution, i.e. equation [5], \n",
    "                        # please observe the relationship between equation [5], [6], and [7]\n",
    "                        full_conditional = ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                        # normalize full_conditional such that it summation equals to 1. \n",
    "                        full_conditional = ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "\n",
    "                        # sample a topic for i-th obserevation of word w in doc d based on full_conditional\n",
    "                        self.z[(d,w,i)] = ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                        \n",
    "                        # update z, doc_topic_matrix, doc_counts, topic_word_matrix, topic_counts here. \n",
    "                        # ## +++++++++ insert code here ++++++++++++++++++++++++###\n",
    "                        ############################################################\n",
    "\n",
    "\n",
    "            # Equation 2  log P(w|z)  for each iteration based on Equation [2]\n",
    "            ## +++++++++ insert code below ++++++++++++++++++++++++###\n",
    "            self.log_likelihood[it]  \n",
    "            ############################################################\n",
    "            print('Iteration %i\\t LL: %.2f' % (it,self.log_likelihood[it]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daZsxFKjJtUD"
   },
   "source": [
    "### Experiment 1: Well separated synthetic dataset. \n",
    "\n",
    "This synthetic dataset consists of 100 documents, and 500 words as total number of vocabulary There are 5 topics in total. \n",
    "\n",
    "In this experiment, you are not required to write any additional code. Run the following code, and see if your topic_word_matrix could resemble true \"phi\", i.e. the true distribution of words in each topic, up to reordering. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qflRoF_ZJuGi"
   },
   "outputs": [],
   "source": [
    "# dataloading \n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "synthetic_data = scipy.io.loadmat('well_separated_synthetic.mat')\n",
    "sk_tdm = scipy.sparse.csr_matrix(synthetic_data['W'])\n",
    "sk_tdm # of shape 100 \\times 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XBLl7f7JzPe"
   },
   "outputs": [],
   "source": [
    "\n",
    "# first run LDA on synthetic data set, \n",
    "# if code works then topic_word_matrix should \n",
    "# resemble true \"phi\", up to reordering\n",
    "\n",
    "# distribution of words in 5 topics, \n",
    "plt.imshow(synthetic_data['phi'],aspect='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om9cCvQp82u1"
   },
   "outputs": [],
   "source": [
    "# run LDA with 5 topics on synthetic data set\n",
    "lda = LDA(sk_tdm, T=5, iteration=100)\n",
    "lda.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb5Nc9IK8-Ax"
   },
   "outputs": [],
   "source": [
    "# compare LDA topic word matrix and true topic word distribution\n",
    "f,a = plt.subplots(1,2)\n",
    "\n",
    "a[0].imshow(lda.topic_word_matrix, aspect='auto')\n",
    "a[1].imshow(synthetic_data['phi'], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zNuNlRMk6vV"
   },
   "source": [
    "### Experiment 2.\n",
    "\n",
    "In this experiment, we use a real-world dataset, with 20 topics (newsgroups). \n",
    "1. you need to build and fit LDA model. \n",
    "2. get top 10 words in each topic, (should make sense) \n",
    "3. plot train loglikelihood, i.e. equation [2] in the reference paper, (should increase over iterations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "TC-hjqim8_Qd",
    "outputId": "5b8a9102-b64f-4b41-fd65-f4d5307bb97e"
   },
   "outputs": [],
   "source": [
    "# data loading \n",
    "# create term document matrix, documents from collection of 20 newsgroups\n",
    "count_vector = CountVectorizer(stop_words=\"english\", \n",
    "                                min_df=.01, \n",
    "                                max_df=.95)\n",
    "data_set = datasets.fetch_20newsgroups()\n",
    "sk_tdm = count_vector.fit_transform(data_set['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsfRA-0K9AkY"
   },
   "outputs": [],
   "source": [
    "# get list of words in corpus\n",
    "corpus_words = np.array(count_vector.get_feature_names())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "FEZHzJXd9Btu",
    "outputId": "7bc8a3ad-97d5-4ccd-f9a7-f13d189373de"
   },
   "outputs": [],
   "source": [
    "# run LDA with 20 topics on 20 news groups dataset\n",
    "lda = LDA(sk_tdm, T=20, iteration=100)\n",
    "lda.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFo5gLfm9DXn"
   },
   "outputs": [],
   "source": [
    "# get top 10 words in each topic, should make sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC76ssEe9ESH"
   },
   "outputs": [],
   "source": [
    "# plot train loglikelihood, should increase over iterations\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "STAT4609_A4 template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
