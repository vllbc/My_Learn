{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依据The Annotated Transformer的代码进行复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.61632543, 3.04326083],\n",
       "       [8.85053312, 3.95206868],\n",
       "       [5.77746852, 3.29248242],\n",
       "       [8.44016844, 3.86399019]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "data = np.array([[1, 2], [5, 4], [2, 3], [5, 2]])\n",
    "cov = np.cov(data.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "eig_vals, eig_vecs\n",
    "idxs = np.argsort(eig_vals)[::-1]\n",
    "topfeat = 1\n",
    "select_vecs = eig_vecs[:, idxs[:topfeat]]\n",
    "low_Data = np.dot(data, select_vecs)\n",
    "# 数据重构\n",
    "reconData = (low_Data * select_vecs.T) + np.mean(data, axis=0)\n",
    "reconData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # 待训练矩阵\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position_embedding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        position_embedding[:, 0::2] = torch.sin(position * div_term)\n",
    "        position_embedding[:, 1::2] = torch.cos(position * div_term)\n",
    "        position_embedding = position_embedding.unsqueeze(0)\n",
    "        # 预留batch size的位置\n",
    "        self.register_buffer('PositionalEncoding', position_embedding)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + Variable(self.PositionalEncoding[:, :x.size(1)], requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # 最后两个维度相乘，即为scores，再scale一下。\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # 将mask的位置的scores置为-1e9\n",
    "    \n",
    "    # 实际上pad mask的时候，pad也会作为key与其它token对应的k,v计算score，pad mask只是消除pad作为k,v时候的影响。但在最后softmax的时候，将pad的损失值全部置为0\n",
    "    p_attn = F.softmax(scores, dim=-1) # 将scores进行softmax，得到p_attn，这里是在最后一个维度上softmax，因为对每个query的所有key进行softmax\n",
    "    if dropout:\n",
    "        p_attn = dropout(p_attn)\n",
    "        \n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1) -> None:\n",
    "        # h为head，这里为8，d_model为embedding的维度，这里为512\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h # 64\n",
    "        self.h = h\n",
    "        self.Q_Linear = nn.Linear(d_model, d_model)\n",
    "        self.K_Linear = nn.Linear(d_model, d_model)\n",
    "        self.V_Linear = nn.Linear(d_model, d_model)\n",
    "        self.res_Linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        batch_size = query.size(0)\n",
    "        query = self.Q_Linear(query).view(batch_size, -1, self.h, self.d_k) # (batch_size, seq_len, h, d_k)即(batch_size, seq_len, 8, 64)\n",
    "        query = query.transpose(1, 2) # (batch_size, h, seq_len, d_k)即(batch_size, 8, seq_len, 64)\n",
    "        \n",
    "        key = self.K_Linear(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        value = self.V_Linear(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # x为(batch_size, h, seq_len, d_k)\n",
    "        # attn为(batch_size, h, seq_len1, seq_len2)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        # (batch_size, h, seq_len, d_k) -> (batch_size, seq_len, h, d_k) -> (batch_size, seq_len, h * d_k) = (batch_size, seq_len, 512)\n",
    "        \n",
    "        return self.res_Linear(x)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # 这里和论文不同，先norm再扔给sublayer（比如多头注意力、ffn）,理论上是self.norm(x+self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feedforward, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feedforward = feedforward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # 这里匿名函数的意义就是sublayer必须是可调用的，如果去掉匿名函数，则就直接计算self_attn(x, x, x, mask)，就是一个结果而不可以调用了\n",
    "        return self.sublayer[1](x, self.feedforward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feedforward, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feedforward = feedforward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, momery, src_mask, tgt_mask):\n",
    "        m = momery\n",
    "        # (batch.size, sequence.len, 512) 来自Encoder的输出 \n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) # 自注意力\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask)) # target和memory的注意力\n",
    "        return self.sublayer[2](x, self.feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, momery, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, momery, src_mask, tgt_mask)\n",
    "        return self.norm(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, momery, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), momery, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    positon = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embedding(d_model, src_vocab), c(positon)),\n",
    "        nn.Sequential(Embedding(d_model, tgt_vocab), c(positon)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p) # 初始化参数\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.0.self_attn.Q_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.Q_Linear.bias torch.Size([512])\n",
      "encoder.layers.0.self_attn.K_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.K_Linear.bias torch.Size([512])\n",
      "encoder.layers.0.self_attn.V_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.V_Linear.bias torch.Size([512])\n",
      "encoder.layers.0.self_attn.res_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.res_Linear.bias torch.Size([512])\n",
      "encoder.layers.0.feedforward.w_1.weight torch.Size([2048, 512])\n",
      "encoder.layers.0.feedforward.w_1.bias torch.Size([2048])\n",
      "encoder.layers.0.feedforward.w_2.weight torch.Size([512, 2048])\n",
      "encoder.layers.0.feedforward.w_2.bias torch.Size([512])\n",
      "encoder.layers.0.sublayer.0.norm.a_2 torch.Size([512])\n",
      "encoder.layers.0.sublayer.0.norm.b_2 torch.Size([512])\n",
      "encoder.layers.0.sublayer.1.norm.a_2 torch.Size([512])\n",
      "encoder.layers.0.sublayer.1.norm.b_2 torch.Size([512])\n",
      "encoder.layers.1.self_attn.Q_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.Q_Linear.bias torch.Size([512])\n",
      "encoder.layers.1.self_attn.K_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.K_Linear.bias torch.Size([512])\n",
      "encoder.layers.1.self_attn.V_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.V_Linear.bias torch.Size([512])\n",
      "encoder.layers.1.self_attn.res_Linear.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.res_Linear.bias torch.Size([512])\n",
      "encoder.layers.1.feedforward.w_1.weight torch.Size([2048, 512])\n",
      "encoder.layers.1.feedforward.w_1.bias torch.Size([2048])\n",
      "encoder.layers.1.feedforward.w_2.weight torch.Size([512, 2048])\n",
      "encoder.layers.1.feedforward.w_2.bias torch.Size([512])\n",
      "encoder.layers.1.sublayer.0.norm.a_2 torch.Size([512])\n",
      "encoder.layers.1.sublayer.0.norm.b_2 torch.Size([512])\n",
      "encoder.layers.1.sublayer.1.norm.a_2 torch.Size([512])\n",
      "encoder.layers.1.sublayer.1.norm.b_2 torch.Size([512])\n",
      "encoder.norm.a_2 torch.Size([512])\n",
      "encoder.norm.b_2 torch.Size([512])\n",
      "decoder.layers.0.self_attn.Q_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.Q_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.self_attn.K_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.K_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.self_attn.V_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.V_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.self_attn.res_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.res_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.Q_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.Q_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.K_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.K_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.V_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.V_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.res_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.res_Linear.bias torch.Size([512])\n",
      "decoder.layers.0.feedforward.w_1.weight torch.Size([2048, 512])\n",
      "decoder.layers.0.feedforward.w_1.bias torch.Size([2048])\n",
      "decoder.layers.0.feedforward.w_2.weight torch.Size([512, 2048])\n",
      "decoder.layers.0.feedforward.w_2.bias torch.Size([512])\n",
      "decoder.layers.0.sublayer.0.norm.a_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.0.norm.b_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.1.norm.a_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.1.norm.b_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.2.norm.a_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.2.norm.b_2 torch.Size([512])\n",
      "decoder.layers.1.self_attn.Q_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.Q_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.self_attn.K_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.K_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.self_attn.V_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.V_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.self_attn.res_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.res_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.Q_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.Q_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.K_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.K_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.V_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.V_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.res_Linear.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.res_Linear.bias torch.Size([512])\n",
      "decoder.layers.1.feedforward.w_1.weight torch.Size([2048, 512])\n",
      "decoder.layers.1.feedforward.w_1.bias torch.Size([2048])\n",
      "decoder.layers.1.feedforward.w_2.weight torch.Size([512, 2048])\n",
      "decoder.layers.1.feedforward.w_2.bias torch.Size([512])\n",
      "decoder.layers.1.sublayer.0.norm.a_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.0.norm.b_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.1.norm.a_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.1.norm.b_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.2.norm.a_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.2.norm.b_2 torch.Size([512])\n",
      "decoder.norm.a_2 torch.Size([512])\n",
      "decoder.norm.b_2 torch.Size([512])\n",
      "src_embed.0.embedding.weight torch.Size([10, 512])\n",
      "tgt_embed.0.embedding.weight torch.Size([10, 512])\n",
      "generator.proj.weight torch.Size([10, 512])\n",
      "generator.proj.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "for name, param in tmp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "    else:\n",
    "        print('no gradient for', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ True, False, False],\n",
      "         [ True,  True, False],\n",
      "         [ True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "print(subsequent_mask(3))\n",
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2) # 这里pad对应的Idx为0,src只需要pad mask，而decoder需要pad和subsequent mask\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1] #decoder 输入\n",
    "            self.trg_y = trg[:, 1:] # label\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        # subsequent_mask的输出是一个下三角矩阵\n",
    "        # 假设&号左边的size为(2, 1, 3)，右边的size为(1, 3, 3)\n",
    "        # 则结果size为(2, 3, 3)\n",
    "        return tgt_mask\n",
    "    \n",
    "# &算子左边，当tgt_mask,shape=(3,1,4)是：\n",
    "# [1 1 1 1], \n",
    "# [1 1 1 0], # attention here\n",
    "# [1 1 1 1]\n",
    "# 类似于第二个序列的最后一个词是'<blank>' 。 \n",
    "\n",
    "# &算子右边，的Variable()内部，shape=(1,4,4)，\n",
    "# 是： \n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 1 \n",
    "# 的时候。\n",
    "\n",
    "# 这两者的&的结果是 (3, 4, 4)： \n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 1 \n",
    "\n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 0 # attention here， 因为第二个序列只有3个词\n",
    "\n",
    "# 1 0 0 0 \n",
    "# 1 1 0 0 \n",
    "# 1 1 1 0 \n",
    "# 1 1 1 1\n",
    "\n",
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(0, V, size=(batch, 10), dtype=np.int64))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_mask = torch.randint(1, 2, (3, 1, 4))\n",
    "scores_mask[:, :, -2:] = 0\n",
    "scores_mask = subsequent_mask(4) & Variable(scores_mask.type(torch.uint8))\n",
    "scores = torch.rand(3, 4, 4)\n",
    "scores.masked_fill(scores_mask == 0, -1e9).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化算法、损失函数、one epoch train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt: # 优化算法\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module): # 标签平滑损失函数\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        #  x的shape为(batch.size * seq.len, target.vocab.size)\n",
    "        # y的shape是(batch.size * seq.len)\n",
    "        \n",
    "        # x=logits，(seq.len, target.vocab.size)\n",
    "        # 每一行，代表一个位置的词\n",
    "        # 类似于：假设seq.len=3, target.vocab.size=5\n",
    "        # x中保存的是log(prob)\n",
    "        #x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        # target 类似于：\n",
    "        # target = tensor([2, 1, 0])，torch.size=(3)\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        # true_dist = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2)) \n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        \n",
    "        # 注意，这里分母target.vocab.size-2是因为\n",
    "        # (1) 最优值 0.6要占一个位置；\n",
    "        # (2) 填充词 <blank> 要被排除在外\n",
    "        # 所以被激活的目标语言词表大小就是self.size-2\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "          # target.data.unsqueeze(1) -> \n",
    "        # tensor([[2],\n",
    "        #[1],\n",
    "        #[0]]); shape=torch.Size([3, 1])  \n",
    "        # self.confidence = 0.6\n",
    "        \n",
    "        # 根据target.data的指示，按照列优先(1)的原则，把0.6这个值\n",
    "        # 填入true_dist: 因为target.data是2,1,0的内容，\n",
    "        # 所以，0.6填入第0行的第2列（列号，行号都是0开始）\n",
    "        # 0.6填入第1行的第1列\n",
    "        # 0.6填入第2行的第0列：\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        \n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        # 设置true_dist这个tensor的第一列的值全为0\n",
    "        # 因为这个是填充词'<blank>'所在的id位置，不应该计入\n",
    "        # 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表\n",
    "        #中的一个词的id\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        \n",
    "         # mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0，\n",
    "        # 因为是'<blank>'的id，所以通过上面的一步，把他们找出来\n",
    "        # 如果不加上nonzero，那么mask的shape就是torch.Size([3])\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "            # 当target reference序列中有0这个'<blank>'的时候，则需要把\n",
    "            # 这一行的值都清空。\n",
    "            # 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要\n",
    "            # pad '<blank>'来填充，所以会出现类似于(2,1,0)这样的情况\n",
    "            # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
    "        # 这一步就是调用KL loss来计算\n",
    "          # x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "          \n",
    "          # true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "\n",
    "class SimpleLossCompute: # 计算损失并反向传播\n",
    "    def __init__(self, generator, criterion, opt=None) -> None:\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "    \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm.item()\n",
    "        loss.backward()\n",
    "        if self.opt:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data.item() * norm.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "def run_epoch(aepoch, data_iter, model, loss_compute):\n",
    "\n",
    "    \"Standard Training and Logging Function\"\n",
    "    # data_iter = 所有数据的打包\n",
    "    # model = EncoderDecoder 对象\n",
    "    # loss_compute = SimpleLossCompute对象\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # 对每个batch循环\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        # 使用目前的model，对batch.src+batch.trg进行forward\n",
    "                            \n",
    "        # e.g.,\n",
    "        # batch.src (2,4) = tensor([[1, 4, 2, 1],\n",
    "        # [1, 4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.trg (2,3) = tensor([[1, 4, 2],\n",
    "        # [1, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.src_mask (2,1,4) = tensor([[[1, 1, 1, 1]],\n",
    "        # [[1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # batch.trg_mask (2,3,3) = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "         #[1, 1, 1]]], dtype=torch.uint8)\n",
    "         \n",
    "        # and out (2,3,8):\n",
    "        # out = tensor([[[-0.4749, -0.4887,  0.1245, -0.4042,  0.5301,  \n",
    "        #   1.7662, -1.6224, 0.5694],\n",
    "        # [ 0.4683, -0.7813,  0.2845,  0.4464, -0.3088, -0.1751, -1.6643,\n",
    "        #   1.7303],\n",
    "         #[-1.1600, -0.2348,  1.0631,  1.3192, -0.9453,  0.3538,  0.7051...                 \n",
    "        \n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        # out和trg_y计算Loss\n",
    "        # ntokens = 6 (trg_y中非'<blank>'的token的个数)\n",
    "        # 注意，这里是token,不是unique word\n",
    "        # 例如[ [ [1, 2, 3], [2,3,4] ]中有6个token,而只有4个unique word\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            \"attention here 这里隐藏一个bug\"\n",
    "            #print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "            #        (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            print ('epoch step: {}:{} Loss: {}/{}, tokens per sec: {}/{}'\n",
    "                    .format(aepoch, i, loss, batch.ntokens, \n",
    "                    tokens, elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'learning rate')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "opts = [NoamOpt(512, 1, 4000, None), \n",
    "        NoamOpt(512, 1, 8000, None),\n",
    "        NoamOpt(256, 1, 4000, None)]\n",
    "plt.plot(np.arange(1, 20000), \n",
    "  [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start model training...\n",
      "epoch=0, training...\n",
      "epoch step: 0:1 Loss: 88.65534043312073/31, tokens per sec: 62/0.28242969512939453\n",
      "evaluating...\n",
      "epoch step: 0:1 Loss: 102.32730031013489/35, tokens per sec: 66/0.060967445373535156\n",
      "tensor(2.8912)\n",
      "epoch=1, training...\n",
      "epoch step: 1:1 Loss: 77.49501848220825/31, tokens per sec: 63/0.09920382499694824\n",
      "evaluating...\n",
      "epoch step: 1:1 Loss: 84.65634942054749/31, tokens per sec: 64/0.05257558822631836\n",
      "tensor(2.6950)\n",
      "epoch=2, training...\n",
      "epoch step: 2:1 Loss: 90.68220734596252/35, tokens per sec: 69/0.07580423355102539\n",
      "evaluating...\n",
      "epoch step: 2:1 Loss: 95.7685604095459/36, tokens per sec: 70/0.04938197135925293\n",
      "tensor(2.8016)\n",
      "epoch=3, training...\n",
      "epoch step: 3:1 Loss: 100.91917181015015/34, tokens per sec: 68/0.07680249214172363\n",
      "evaluating...\n",
      "epoch step: 3:1 Loss: 83.68315076828003/34, tokens per sec: 65/0.05086374282836914\n",
      "tensor(2.7281)\n",
      "epoch=4, training...\n",
      "epoch step: 4:1 Loss: 93.78761315345764/33, tokens per sec: 67/0.07280731201171875\n",
      "evaluating...\n",
      "epoch step: 4:1 Loss: 80.1437520980835/31, tokens per sec: 64/0.048876047134399414\n",
      "tensor(2.7962)\n",
      "epoch=5, training...\n",
      "epoch step: 5:1 Loss: 80.39596557617188/32, tokens per sec: 65/0.07956457138061523\n",
      "evaluating...\n",
      "epoch step: 5:1 Loss: 77.84976482391357/30, tokens per sec: 64/0.04983162879943848\n",
      "tensor(2.5845)\n",
      "epoch=6, training...\n",
      "epoch step: 6:1 Loss: 90.27347016334534/33, tokens per sec: 64/0.07081174850463867\n",
      "evaluating...\n",
      "epoch step: 6:1 Loss: 87.2994384765625/32, tokens per sec: 64/0.04470324516296387\n",
      "tensor(2.5351)\n",
      "epoch=7, training...\n",
      "epoch step: 7:1 Loss: 75.774658203125/32, tokens per sec: 60/0.08058738708496094\n",
      "evaluating...\n",
      "epoch step: 7:1 Loss: 96.8997573852539/36, tokens per sec: 69/0.05186104774475098\n",
      "tensor(2.6010)\n",
      "epoch=8, training...\n",
      "epoch step: 8:1 Loss: 86.1125693321228/33, tokens per sec: 66/0.07779383659362793\n",
      "evaluating...\n",
      "epoch step: 8:1 Loss: 88.3519515991211/32, tokens per sec: 64/0.04592585563659668\n",
      "tensor(2.5940)\n",
      "epoch=9, training...\n",
      "epoch step: 9:1 Loss: 79.15554213523865/33, tokens per sec: 68/0.0791933536529541\n",
      "evaluating...\n",
      "epoch step: 9:1 Loss: 83.3953857421875/30, tokens per sec: 64/0.04747176170349121\n",
      "tensor(2.6911)\n"
     ]
    }
   ],
   "source": [
    "# Train the simple copy task.\n",
    "V = 11 # here V is the vocab size of source and target languages (sequences)\n",
    "criterion = LabelSmoothing(size=V, \n",
    "    padding_idx=0, smoothing=0.01) # 创建损失函数计算对象\n",
    "    \n",
    "model = make_model(V, V, N=2, d_model=8, d_ff=16, h=2) \n",
    "# EncoderDecoder对象构造\n",
    "'''\n",
    "in make_model: src_vocab_size=11, tgt_vocab_size=11, \n",
    "    N=2, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "'''\n",
    "\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), \n",
    "        lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "# 模型最优化算法的对象\n",
    "\n",
    "lossfun = SimpleLossCompute(model.generator, \n",
    "        criterion, model_opt)\n",
    "if True:\n",
    "    print ('start model training...')\n",
    "    for epoch in range(10):\n",
    "        print ('epoch={}, training...'.format(epoch))\n",
    "        model.train() # set the model into \"train\" mode\n",
    "        # 设置模型进入训练模式\n",
    "        \n",
    "        #lossfun = SimpleLossCompute(model.generator, \n",
    "        #    criterion, model_opt) # 不需要在这里定义lossfun\n",
    "        \n",
    "        run_epoch(epoch, data_gen(V, 4, 2), model, lossfun)\n",
    "        # 重新构造一批数据，并执行训练\n",
    "        \n",
    "        model.eval() # 模型进入evaluation模式 (dropout，反向传播无效）\n",
    "        print ('evaluating...')\n",
    "        print(run_epoch(epoch, data_gen(V, 4, 2), model, \n",
    "                        SimpleLossCompute(model.generator, \n",
    "                        criterion, None)))\n",
    "        # 这里None表示优化函数为None，所以不进行参数更新 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试过程（贪心搜索）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=tensor([[[-0.0074, -0.5101,  0.9849,  1.7664,  0.3676, -0.3998, -1.1558,\n",
      "          -1.0329],\n",
      "         [-0.1066, -0.4749, -0.6959,  1.8062,  1.3188, -0.7176, -0.2250,\n",
      "          -0.8860],\n",
      "         [ 0.2270, -1.0549,  0.5829,  1.4915,  0.6446, -0.4115,  0.1804,\n",
      "          -1.6517],\n",
      "         [ 0.1524, -0.9586,  0.1945,  1.6614,  1.0826, -1.3099, -0.1595,\n",
      "          -0.6519]]], grad_fn=<AddBackward0>), memory.shape=torch.Size([1, 4, 8])\n",
      "ys=tensor([[1]]), ys.shape=torch.Size([1, 1])\n",
      "out.shape=torch.Size([1, 1, 8])\n",
      "next_word=1\n",
      "out.shape=torch.Size([1, 2, 8])\n",
      "next_word=1\n",
      "out.shape=torch.Size([1, 3, 8])\n",
      "next_word=1\n",
      "out.shape=torch.Size([1, 4, 8])\n",
      "next_word=1\n",
      "tensor([[1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask) \n",
    "    # 源语言的一个batch\n",
    "    # 执行encode编码工作，得到memory \n",
    "    # shape=(batch.size, src.seq.len, d_model)\n",
    "    \n",
    "    # src = (1,4), batch.size=1, seq.len=4\n",
    "    # src_mask = (1,1,4) with all ones\n",
    "    # start_symbol=1\n",
    "    \n",
    "    print ('memory={}, memory.shape={}'.format(memory, \n",
    "        memory.shape))\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 最初ys=[[1]], size=(1,1); 这里start_symbol=1\n",
    "    print ('ys={}, ys.shape={}'.format(ys, ys.shape))\n",
    "    for i in range(max_len-1): # max_len = 5\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        # memory, (1, 4, 8), 1=batch.size, 4=src.seq.len, 8=d_model\n",
    "        # src_mask = (1,1,4) with all ones\n",
    "        # out, (1, 1, 8), 1=batch.size, 1=seq.len, 8=d_model                             \n",
    "        # print ('out={}, out.shape={}'.format(out, out.shape))\n",
    "        print('out.shape={}'.format(out.shape))\n",
    "        prob = model.generator(out[:, -1]) \n",
    "        # pick the right-most word\n",
    "        # (1=batch.size,8) -> generator -> prob=(1,5) 5=trg.vocab.size\n",
    "        # -1 for ? only look at the final (out) word's vector\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        print('next_word={}'.format(next_word))\n",
    "        # word id of \"next_word\"\n",
    "        ys = torch.cat([ys, \n",
    "          torch.ones(1, 1).type_as(src.data).fill_(next_word)], \n",
    "          dim=1)\n",
    "        # ys is in shape of (1,2) now, i.e., 2 words in current seq\n",
    "    return ys\n",
    "\n",
    "if True:\n",
    "    model.eval()\n",
    "    src = Variable(torch.LongTensor([[1,2,3,4]]))\n",
    "    src_mask = Variable(torch.ones(1, 1, 4))\n",
    "    print(greedy_decode(model, src, src_mask, max_len=5, \n",
    "        start_symbol=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
